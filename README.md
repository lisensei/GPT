Train a GPT from scratch

Both torch's transformer encoder and decoer layers can be used as GPT layer. The encoder layer can directly be used since it has no cross attention layer and also accepts attention mask. Using the decoder layer as GPT layer requires modification to the cross attention layer.

When trained on Plato's *The Republic*, here is a sample generated text using prompt "I need another story":

*I need another story much aged on by the poet of the world, then
be a danger of our guardian or guardians are always by us to began
who would not be can educated? In cannot pleasure only weden to a
sense of a perfect State?* 

*There can be no doubt den, and the nature of justice and virtue, and
the word which follow and have to do not the human body; and that
he had not been a pauper of him a beauty of peace among people and
when they are formed and free and beauty as well as by nature --and
the appear to you?* 

*Yes, he replied; I agree wife do not be satisfactory of them: and
he who desire to another men and to a corrupted by us just and not
a far judge one another' and would not here under a real and honours
of them are children.* 

*That was not of poetry cannot be more referred to be death or knowledge
or hear no chrow their own country, and then we would be compelled
to fear when we have described how wanded never ever been deemed who have
come freed the just man or unjust and accord*